{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70o2abDMjbNN"
   },
   "source": [
    "We use Unsloth since it handles the conversion to GGUF for Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7-Fu_XVehrnt",
    "outputId": "0b76a075-b1d8-4107-b24b-5d1e6bdbd54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-dcm07rl6/unsloth_7af5d47b0a614fee9c94de005bc889b7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-dcm07rl6/unsloth_7af5d47b0a614fee9c94de005bc889b7\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit ec1757c1a02175851146ff5f6ab2a26c8c863fc8\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting unsloth_zoo>=2026.1.2 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading unsloth_zoo-2026.1.2-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\n",
      "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-1.0.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.57.3)\n",
      "Collecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.29.5)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
      "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.7.0)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2026.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.12.0)\n",
      "Collecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth_zoo>=2026.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.18.0)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2026.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2026.1.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.11.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.3)\n",
      "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2026.1.2-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-1.0.3-py3-none-any.whl (180 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m149.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for unsloth: filename=unsloth-2026.1.2-py3-none-any.whl size=392996 sha256=8b155054c95f959395b4536a95e8c328a75d21fa2a95504b71c6e3e5ecd0fe30\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2d9spkfm/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
      "Successfully built unsloth\n",
      "Installing collected packages: torchao, unsloth, pyarrow, msgspec, tyro, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "Successfully installed bitsandbytes-0.49.1 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.20.0 pyarrow-22.0.0 torchao-0.15.0 trl-0.24.0 tyro-1.0.3 unsloth-2026.1.2 unsloth_zoo-2026.1.2\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.24.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
      "Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xformers\n",
      "Successfully installed xformers-0.0.33.post2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483,
     "referenced_widgets": [
      "12df6c8cbea242aeac533acd91db1524",
      "62b00978b8884fd989946740a113cdec",
      "89ee1e7765a74e859f1225e9358a6332",
      "ce6c8a1d5ab44a93aaa7fc6f9a1d78d7",
      "6af1a599266f4e249aa048a7ef9dda25",
      "e6e66c99df954d39b72deb771223e31b",
      "b1d56b68fe99405c8b37a0097d64a223",
      "bb5efa8510ce43bfaa90dcc034e1b19d",
      "9baa0230d4094bc1ab1cffbb2d0da352",
      "01c11faccb324f3ba8b8d47a7292804f",
      "e76e317852ec4c60a28ebc77b434a813",
      "ea15609efed64cc49701abfbaa765622",
      "fe163d547b3e41a0b577d1c6446a3aa5",
      "84132d4a59f24af28a65343a42d4cee5",
      "64560baa94b247c99dd81394633d9d0f",
      "6306466760eb467c91f7ea0051a7bbb0",
      "1a991c0ec68c4fc9a2e1d362d3cb4f29",
      "7250f885ac6e45aa8152f663e24ed4a4",
      "c106b3805de243dca433e69be05fe33b",
      "b2d4352fe5fc4481a87301991ff68ac1",
      "dfad787eb47c4efd8b60341d99f87dcd",
      "0dbc8b7f8cf34d1ab5dd62c82a9a0b49",
      "c298e36abfef4c87a5d4aaa425ba6359",
      "f57fd72d8a9a4dc78d3a3f9f9db2b09a",
      "e7c0ba1fefb440a2874dbbe64de1d92d",
      "7009c672cc2e46439fac76e8834d3ca3",
      "9e54c1e4941d44cf85fc2550fa2d5ad4",
      "07346afc079e48afb9b8c0ecdb3fc199",
      "916fd1144c3d42a5985897c615c8d0f1",
      "da4f435120394c58a25eb73d4dfe1961",
      "91c5f4cf6e424fb782dc489e61f49ec0",
      "cab600d9b94a4f5b98c32d96a454c951",
      "107224e2d8b3414f9137f051170fc159",
      "96b015e5165f487f9100d8d4e2c74c6d",
      "8d71f6e6f12d471ab812c6c30c3911c1",
      "541fba23552a44eb9f094f9183260364",
      "8288d753200440669e989c4ea562a0e3",
      "e5c2c9a5f46f475092f1493599a5a915",
      "c90239990cbf4f7b9ea2b355246d668c",
      "f9abbdb878ad4bfa95cb59152947a4f1",
      "bc8a63a33b684d319bb8e7f5293ddccd",
      "b0be3e0155854067a7b8fb111da914a2",
      "1fe3bdb449894f17a8e031e82504b357",
      "90af6e3ca50c4dd3a9beef4f75d49ffe",
      "4fd64561a681409b8c5b550648c07402",
      "cbf198e213aa49b3955abb7ecd64a508",
      "9411e9b0911e4908ab2245e716dd77e6",
      "45dea5b230f141c28123196337e0667b",
      "2c49de83345b45e2ae6e0e4454a7a810",
      "cc699903bbd342b28ae3d5435ac47aab",
      "13b0b0dac8504aef98da4b58488807b5",
      "56c2e5dd2fce46b9ba6005cbb2d16505",
      "81294930876048528ecba21d819482fa",
      "6c0f93ff960f45cc90e193418d59fc03",
      "6de0bb2cea8a4d678d122540235632bd",
      "3b9237990d83447aaca125416d8a813c",
      "15313b8858ae49a0b881d6485aee2844",
      "257c8111866e4bd8a8d57cc7ca2659a5",
      "cfae24af5228423ba15f6576e6089a84",
      "8a8fce1e253b48d595c5772197116d5f",
      "e648b3370b8544f2b394e01c2e900b99",
      "2b952e8cbf304d6dbbc0293bb115b736",
      "9ae5ceeea04d42cf9d0a18edbbceb73d",
      "84296add69f141dc8bcfe377c510b9fb",
      "220f3beae6554c5e948c063e11b6a303",
      "1fbb5399087142979faa840bad9392e6"
     ]
    },
    "collapsed": true,
    "id": "TYeetayRHSmZ",
    "outputId": "e007e73f-4184-4a88-a001-e8e5d077d8ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Loading Llama-3.2-3B-Instruct...\n",
      "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12df6c8cbea242aeac533acd91db1524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea15609efed64cc49701abfbaa765622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c298e36abfef4c87a5d4aaa425ba6359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b015e5165f487f9100d8d4e2c74c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd64561a681409b8c5b550648c07402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9237990d83447aaca125416d8a813c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nuse_gradient_checpointing = \"unsloth\":\\nWe do two passes when training a neural network: Forward pass (data flows through the model, layer by layer, to produce an output)\\nand backward pass (the error is sent back through the model to update the weights)\\nTo do backward pass, GPU needs to remember exactly what the data looked like at every single layer during forward process. These are called activations\\nand storing these activations for a large model eats up GB of VRAM\\nSolution: we use checkpointing. It tells GPU to don\\'t memorize everything, just memorize a few checkpoints.\\nWe save a lot of memory, but training is slower\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load Model\n",
    "# unsloth: open-source library to make fine-tuning LLMs (vs. Hugging Face, unsloth 2x to 30x more faster, reduces VRAM by 60-80%)\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "max_seq_length = 2048 # Context window -> model will remember 2048 tokens (1500 words) at a time\n",
    "dtype = None  # data type for calculations, None allows Unsloth to auto-detect the GPU\n",
    "load_in_4bit = True # Will use Quantization\n",
    "# Quantization -> instead of loading model params as 16-bit floating point numbers, it compresses them into 4-bit integers\n",
    "# So, the model tes up 4 times less RAM, to fit on free Google Colab GPUs\n",
    "\n",
    "print(\"Loading Llama-3.2-3B-Instruct...\")\n",
    "\n",
    "\"\"\"\n",
    "Unsloth -> Handles RoPE Scaling (Rotary Positional Embeddings) automatically;\n",
    "           if you load a model designed for 4k context but ask for 8k, it\n",
    "           mathematically stretches model capacity to handle it\n",
    "FastLanguageModel.from_pretrained():\n",
    "  - Acts as a specialized wrapper around Hugging Face standard loading methods, but\n",
    "  with heavy optimizations during loading process\n",
    "  - Why use this instead of AutoModelForCausalLM? It's the standard Hugging Face\n",
    "  AutoModel class, it loads the standard, slow version of the model\n",
    "\n",
    "Llama-3.2-3B-Instruct:\n",
    " - 3 Billion params\n",
    " - Instruct: already fine-tuned to follow chat instructions\n",
    "\n",
    "Tokenizer: tool that converts text (strings) into numbers (tokens) that model understands\n",
    "\"\"\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\"\"\"\n",
    "FastLanguageModel outputs the following:\n",
    "  1. model: Actual Neural Network -> it's just a \"Base Model\"; it contains knowledge from\n",
    "                              its original training, but it's read-only until we add\n",
    "                              adapters\n",
    "  2. tokenizer: Translator -> it converts human text into numbers that the model understands\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Add LoRA adapters -> Lo-Rank Adapdation\n",
    "Llama is like a giant 1,000-page textbook; Traditional Fine-Tuning implies rewriting the entire textbook to add new information. Slow and expensive\n",
    "LoRA (Adapters) is like leaving the textbook exactly as it is and instead we add notes on specific pages with new updates\n",
    "Slice of the model that sits on top of the frozen giant brain\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "get_peft_model(...):\n",
    "  - instead of retraining all 3 billion params, we freeze main model and attach tiny trainable matrices (adapters)4\n",
    "\"\"\"\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,   # Rank\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,    # Determines how loudly the fine-tuned adapter speaks, alpha / r = 1 mean model blends new data perfectly with old data\n",
    "    lora_dropout = 0,   # Dropout -> technique used to stop the model from memorizing data (overfitting), set to 0 since with LoRA, unecessary\n",
    "    bias = \"none\",   # forbidden to touch the bias (starting point); It adds extra memory overhead and zero noticeable improvement in model quality with LoRA\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\"\"\"\n",
    "use_gradient_checpointing = \"unsloth\":\n",
    "We do two passes when training a neural network: Forward pass (data flows through the model, layer by layer, to produce an output)\n",
    "and backward pass (the error is sent back through the model to update the weights)\n",
    "To do backward pass, GPU needs to remember exactly what the data looked like at every single layer during forward process. These are called activations\n",
    "and storing these activations for a large model eats up GB of VRAM\n",
    "Solution: we use checkpointing. It tells GPU to don't memorize everything, just memorize a few checkpoints.\n",
    "We save a lot of memory, but training is slower\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "0FZ7lP9u8cZc",
    "outputId": "a08fd3cd-cb36-4055-c80a-ea31f84551b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting HuggingFace\n",
      "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: HuggingFace\n",
      "Successfully installed HuggingFace-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "collapsed": true,
    "id": "v2GNrcwj8oOK",
    "outputId": "d78518b1-a727-4075-8c78-1c23dd591bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select your CSV file from your computer...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-42f5bbc2-02e7-4004-9dfd-04e37ad27bee\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-42f5bbc2-02e7-4004-9dfd-04e37ad27bee\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving github_startups.csv to github_startups.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "print(\"Please select your CSV file from your computer...\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379,
     "referenced_widgets": [
      "2b17a10c3efd44dba52bc8f039c387c9",
      "dc28a1b79bd44c098d13d7982b5476f2",
      "8a440d1ae9984a09bbae4c496ec1f10a",
      "0a0f6fc93e6745e49fe89328b8db95e5",
      "77b03939e2644138a4e54f1d68d9ff64",
      "38ced10753f646dbb8b427de190e314c",
      "5964acebc31c4d509dfe5486f0bdfd70",
      "148960d18e564860aebad850d8a626dd",
      "6f287ffb92ff4104aaddad854e9cf481",
      "441a8ec1d0c04ef5b3f85b9e58e28842",
      "8d325d32393c4684bd96668ae72d748b"
     ]
    },
    "collapsed": true,
    "id": "DyONgovfs6m5",
    "outputId": "4890a925-441c-4461-cfd4-786808b1074b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded github_startups.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b17a10c3efd44dba52bc8f039c387c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data fed to model:\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Analyze this startup description and generate a structured Investment Memo.\n",
      "\n",
      "### Input:\n",
      "An Open Source Machine Learning Framework for Everyone\n",
      "\n",
      "### Response:\n",
      "\n",
      "**Problem:** Need to infer from text (Synthesized).\n",
      "**Solution:** Solution based on An Open Source Machi...\n",
      "**Tech Stack:** Extracted relevant tech.\n",
      "**Verdict:** AUTOMATED ANALYSIS\n",
      "  <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3: prepare data\n",
    "Need to turn csv file into a specific \"Prompt Format\"\n",
    "Model needs to see: Instruction + Input + Expected Output\n",
    "\"\"\"\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "try:\n",
    "  df = pd.read_csv(\"github_startups.csv\")\n",
    "  print(\"Loaded github_startups.csv\")\n",
    "except:\n",
    "  print(\"github_startups.csv not found. Generating dummy data for demonstration...\")\n",
    "  data = {\n",
    "      'description': [\n",
    "          \"We are building a decentralized exchange on Solana using Rust. Our TPS is 50k and we use Serum for liquidity.\",\n",
    "          \"An AI tool that uses PyTorch to detect cancer in X-rays.\",\n",
    "          \"A React-based dashboard for tracking carbon emissions.\"\n",
    "      ] * 10\n",
    "  }\n",
    "  df = pd.DataFrame(data)\n",
    "\n",
    "# Because we don't have real \"Investment Memos\", we will synthetically format the\n",
    "# output to teach the model the structure we want\n",
    "def format_dataset(row):\n",
    "  instruction = \"Analyze this startup description and generate a structured Investment Memo.\"\n",
    "  input_text = row['description']\n",
    "\n",
    "  # We create a synthetic \"target\" (This is what the model tries to learn)\n",
    "  # We force the model to learn this specific Markdown structure\n",
    "  output_text = f\"\"\"\n",
    "**Problem:** Need to infer from text (Synthesized).\n",
    "**Solution:** Solution based on {input_text[:20]}...\n",
    "**Tech Stack:** Extracted relevant tech.\n",
    "**Verdict:** AUTOMATED ANALYSIS\n",
    "  \"\"\"\n",
    "\n",
    "  # Combine into full prompt\n",
    "  text = alpaca_prompt.format(instruction, input_text, output_text) + tokenizer.eos_token\n",
    "  return {\"text\": text}\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(format_dataset, batched=False)\n",
    "\n",
    "print(\"Sample Data fed to model:\")\n",
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7eabd99fa42f42d990273a552b623106",
      "495b1f3b8fe147e38d3a3c1d709de6c6",
      "8f0ab9dec6104e2e9d4fce7f3c920575",
      "500b3288b91a4b3bbfdc2334cef67627",
      "2c8d8b6e2b824763aa674a97e2f22a74",
      "9d08d96baf0842f4a38148a431d2b056",
      "da8c5f7ef3674243a35b12c9c5c34270",
      "93a1b7e1c50d441d9efdd43dda924a18",
      "68ed12800f3947699ff6bd9fffa2d101",
      "2ba45cb600a04299990076631f04cc6c",
      "6abe0e6127f6446cae036423d953b8bd"
     ]
    },
    "collapsed": true,
    "id": "BfcSQtQdV3x1",
    "outputId": "3666cb79-6acb-4e07-b5a6-a6b8e1799125"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eabd99fa42f42d990273a552b623106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 5 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n",
      "wandb: (1) Create a W&B account\n",
      "wandb: (2) Use an existing W&B account\n",
      "wandb: (3) Don't visualize my results\n",
      "wandb: Enter your choice:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: You chose \"Don't visualize my results\"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20260111_003609-jt6j6pqq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Detected [huggingface_hub.inference, openai] in use.\n",
      "wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 04:04, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.870800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.794800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.986300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.882200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.914600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.639800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.865900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.640100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.772600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.558400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.753500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.648500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.516100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.579700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.629100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.461400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.733300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING URL not available in offline run\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–…â–…â–…â–…â–†â–†â–…â–‡â–‡â–ˆâ–‡â–…â–„â–„â–‚â–‚â–‚â–‚â–ƒâ–‚â–„â–…â–…â–…â–„â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚</td></tr><tr><td>train/learning_rate</td><td>â–â–‚â–„â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1012379993677824.0</td></tr><tr><td>train/epoch</td><td>4.64</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train/grad_norm</td><td>0.51938</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5385</td></tr><tr><td>train_loss</td><td>1.2297</td></tr><tr><td>train_runtime</td><td>308.5154</td></tr><tr><td>train_samples_per_second</td><td>1.556</td></tr><tr><td>train_steps_per_second</td><td>0.194</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br><code>wandb sync /content/wandb/offline-run-20260111_003609-jt6j6pqq<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20260111_003609-jt6j6pqq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the model (Fine-tuning)\n",
    "\n",
    "# SFT: Supervised Fine-Tuning\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,    # Can speed up training for short sequences\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,    # Small number for demo, increase to 300+ for real results\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 893,
     "referenced_widgets": [
      "33c9029c846546f29b8cf956a31efe32",
      "b4383b17a1854ec79c0d2634992c9835",
      "1edcbf79997e42cea79ca6f82e60f71a",
      "7686a098055744c19d5058449d5daf01",
      "6ef633030b2f460cad6099030c51a640",
      "404012d5a6a1458a9c0b19367c928af9",
      "1840e7561b814959b8eba82d150a3dc9",
      "ccc174d396444a3eb4aae9c5efd45287",
      "64ad00d2507b48ba8499b86bea767305",
      "2399a5bd1eff44ab8ccdd7ce2a3e1023",
      "f98480d1b5a947e09e86a867ed24e918",
      "9e0182afa8724d64bb805af26689a026",
      "81153afa17144c629581348d6462bdb4",
      "646058f3b7eb434185ffb1471d8b1793",
      "802bc16bcade4d81807fd81d9b8668d6",
      "25f94856594e4609a03870c3157eac24",
      "c504817b0ed74d7b96f386dda24e1970",
      "0731ec48916541ed9a9b1527aeaff1d2",
      "fce578384e2e44d4b254cf714588916b",
      "092394759a204c87aed1983765103997",
      "06c76d7bc57146fab938878d032bff4a",
      "be4d464c06564e55bea4cbd49aa4f2d3",
      "c56583d420a6464198e210165afc2eb5",
      "67d83508fff041be9f441fe1560c2c7b",
      "b60af7b44c0244469f1630bf60d86128",
      "30e754e5e18844b5a33672ca96f2ab08",
      "794235ee26f3428c9c14c846172ac485",
      "86fad1fc934c4da7b4fc99d45c4c724f",
      "036d3d93169a4a49a4250012d6dc39a0",
      "4d10b87805a44efdbbf3b22856bde394",
      "791b85f7020e40f9ab7f2b6474708805",
      "e0905a7a0f7b4df38bc7283918d23b02",
      "627a69acea574cd887933bf82d93b845",
      "dfa064104aeb40579a6c127771a7f674",
      "05baa0d018214f298a15f43e52bd1b10",
      "c009419b6bb74be786bd0a5a30b30be5",
      "ed451e555f5f404fa3f373e2f9ec37a4",
      "1071fa8165cc443db6c94e8df814c6af",
      "476e21ac20ab45ce98ace0a765b15393",
      "a078df6aa0104082b1351fa4da73d357",
      "1efc789a65054ca3afbe0898b76444ce",
      "ab0f8bd47bfd428681a4ae04324bc372",
      "3a33d4683434480699c2e8688cdaf5c3",
      "946d3ff5c272421d896bc413810aa1b7"
     ]
    },
    "collapsed": true,
    "id": "j478Js2RgsNQ",
    "outputId": "2595c380-7288-4325-e7a8-e06a8c0e7e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to GGUF format...\n",
      "Unsloth: Merging model weights to 16-bit format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c9029c846546f29b8cf956a31efe32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0182afa8724d64bb805af26689a026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56583d420a6464198e210165afc2eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Unsloth: Preparing safetensor model files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [01:25<01:25, 85.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa064104aeb40579a6c127771a7f674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:05<00:00, 62.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:08<00:00, 94.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/content/model_pi`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF f16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: Updating system package directories\n",
      "Unsloth: All required system packages already installed!\n",
      "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
      "Unsloth: Cloning llama.cpp repository\n",
      "Unsloth: Install GGUF and other packages\n",
      "Unsloth: Successfully installed llama.cpp!\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into f16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['llama-3.2-3b-instruct.F16.gguf']\n",
      "Unsloth: [2] Converting GGUF f16 into q4_k_m. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['llama-3.2-3b-instruct.Q4_K_M.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model llama-3.2-3b-instruct.Q4_K_M.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n",
      "Download the model.pi folder\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Export to GGUF (For Raspberry Pi)\n",
    "# Converts the model to a format the Pi can run\n",
    "\n",
    "print(\"Saving to GGUF format...\")\n",
    "model.save_pretrained_gguf(\"model_pi\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "print(\"Download the model.pi folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_o22VAWwImg",
    "outputId": "2e7eb6f8-7b5f-4eab-b546-2b42ce9b339f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "L_9hjAkAwWUz",
    "outputId": "c2678040-5afe-451b-d957-4dba369c2317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying llama-3.2-3b-instruct.Q4_K_M.gguf to Google Drive...\n",
      "This may take 1-2 minutes...\n",
      "SUCCESS! File copied to: /content/drive/MyDrive/Startup_Model/llama-3.2-3b-instruct.Q4_K_M.gguf\n",
      "Go to drive.google.com now to download it.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "source_folder = \"/content\"\n",
    "destination_folder = \"/content/drive/MyDrive/Startup_Model\"\n",
    "\n",
    "# Create the destination folder in your Drive\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# Find the file and copy it\n",
    "try:\n",
    "    files = [f for f in os.listdir(source_folder) if f.endswith('.gguf')]\n",
    "    if files:\n",
    "        filename = files[0]\n",
    "        src_path = os.path.join(source_folder, filename)\n",
    "        dst_path = os.path.join(destination_folder, filename)\n",
    "\n",
    "        print(f\"Copying {filename} to Google Drive...\")\n",
    "        print(\"This may take 1-2 minutes...\")\n",
    "\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        print(f\"SUCCESS! File copied to: {dst_path}\")\n",
    "        print(\"Go to drive.google.com now to download it.\")\n",
    "    else:\n",
    "        print(\"ERROR: No .gguf file found in model_pi. Did the saving step finish?\")\n",
    "except Exception as e:\n",
    "  print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
